name: Process OSN Bucket Data with Docker Model

on:
  schedule:
    - cron: '0 0 * * *'  # Every night at midnight UTC
  workflow_dispatch:  # Allow manual triggering

jobs:
  process_data:
    runs-on: ubuntu-latest
    
    steps:
      # Step 1: Checkout repository (if any code changes are needed)
      - name: Checkout code
        uses: actions/checkout@v4

      # Step 3: Build docker image for bat-detect-msds
      - name: Build bat-detect-msds image
        run: docker build -t bat-detect-msds -f ./bat-detect-msds/Dockerfile .

      #- name: Build buzzfindr image
      #  run: docker build -t buzzfindr-image -f ./buzzfindr/Dockerfile .

      # Step 2: Install rclone and configure it for OSN access
      - name: Set up rclone for OSN access
        run: |
          sudo apt-get update
          sudo apt-get install -y rclone
          mkdir -p ~/.config/rclone
          echo "[osn_sdsc_ubna]" > ~/.config/rclone/rclone.conf
          echo "type = s3" >> ~/.config/rclone/rclone.conf
          echo "provider = Ceph" >> ~/.config/rclone/rclone.conf
          echo "access_key_id = " >> ~/.config/rclone/rclone.conf  # Leave blank for public access
          echo "secret_access_key = " >> ~/.config/rclone/rclone.conf  # Leave blank for public access
          echo "endpoint = https://sdsc.osn.xsede.org" >> ~/.config/rclone/rclone.conf
          echo "no_check_bucket = true" >> ~/.config/rclone/rclone.conf

      # Step 3: Download the latest directory info artifact (if exists)
      - name: Download latest directory info artifact (if exists)
        uses: actions/download-artifact@v4
        with:
          name: latest-directory-artifact
          path: ./latest_directory.txt
        continue-on-error: true  # If the artifact doesn't exist, continue without error

      # Step 4: Check for the most recent directory in the OSN bucket
      - name: Check for most recent directory in OSN bucket
        id: check_data
        run: |
          # List directories in the OSN bucket using rclone
          NEW_DATA=$(rclone lsd osn_sdsc_ubna:bio230143-bucket01 | sort | tail -n 1| awk '{print $NF}')
          echo "Most recent directory: $NEW_DATA"

          # Default latest directory (if no previous directory file exists)
          LAST_PROCESSED_DIRECTORY="ubna_data_04"

          # If previous directory exists in latest_directory.txt, use it
          if [[ -f latest_directory.txt ]]; then
            LAST_PROCESSED_DIRECTORY=$(cat latest_directory.txt)
          fi

          # Compare the most recent directory to the last processed directory
          if [[ "$NEW_DATA" == "$LAST_PROCESSED_DIRECTORY" ]]; then
            echo "No new data found"
            echo "::set-output name=new_data::false"
          else
            echo "New data detected"
            echo "::set-output name=new_data::true"
            echo "::set-output name=new_data_path::osn_sdsc_ubna:bio230143-bucket01/${NEW_DATA}"

            # Update the latest processed directory
            echo "$NEW_DATA" > latest_directory.txt
          fi

      # Step 5: Mount the OSN bucket and process data in Docker
      - name: Mount OSN bucket and process data in Docker
        if: steps.check_data.outputs.new_data == 'true'
        run: |
            mkdir -p /tmp/osn_bucket/
            chmod 777 /tmp/osn_bucket/

            #NEW_DATA=$(rclone lsd osn_sdsc_ubna:bio230143-bucket01 | sort | tail -n 1| awk '{print $NF}')

            sudo sed -i '/^#user_allow_other/s/^#//g' /etc/fuse.conf

            # Step 2: Unmount the existing FUSE filesystem (if mounted)
            #fusermount -u /tmp/osn_bucket/ || true

            echo "after fusermount"

            # Step 3: Mount the FUSE filesystem with rclone using allow_other
            #rclone mount osn_sdsc_ubna:bio230143-bucket01/ubna_data_02/recover-20230622/UBNA_007/ /tmp/osn_bucket/ --vfs-cache-mode writes --log-level DEBUG 
            rclone mount osn_sdsc_ubna:bio230143-bucket01/ubna_data_02/recover-20230622/UBNA_007/ /tmp/osn_bucket/ \
            --vfs-cache-mode off \
            --log-level DEBUG \
            --allow-other \
            --daemon

            echo "mounted"

            sleep 100  # Adjust sleep time as necessary to give rclone time to mount
  run_gpu_job: 
    runs-on: self-hosted
    steps:
      - name: Run Docker container with mounted directories
        run: |
            echo "before container run"
            # Step 7: Run the Docker container with the mounted OSN bucket file
            #docker run --mount type=bind,source=/tmp/osn_bucket/,dst=/app/recordings_2023/ buzzfindr-image:latest
            #docker run -v /tmp/osn_bucket/:/app/recordings_2023/ buzzfindr-image:latest
            
            docker run --gpus all --mount type=bind,source=/tmp/osn_bucket/,target=/app/recordings_2023/ \
            --mount type=bind,source=./bat-detect-msds/output_dir/,target=/app/output_dir/ \
            bat-detect-msds:latest python3 ./bat-detect-msds/src/batdt2_pipeline.py --input_audio='/app/recordings_2023/' \
            --output_directory='/app/output_dir/' --run_model --csv

            # Clean up (optional, unmount the rclone mount)
            umount /tmp/osn_bucket/

      # Step 6: Upload the latest directory info as an artifact for next run
      - name: Upload latest directory info artifact
        uses: actions/upload-artifact@v4
        with:
          name: latest-directory-artifact
          path: latest_directory.txt
          
      # Step 7: Move the output to Manila folder on Jetstream2
      - name: Move output to Manila folder on Jetstream2
        if: steps.check_data.outputs.new_data == 'true'
        run: |
          sshpass -p "${{ secrets.JETSTREAM_PASSWORD }}" scp -r ~/osn_bucket/output/ user@jetstream2:/path/to/manila/folder/
          echo "Output moved to Jetstream2 manila folder"
