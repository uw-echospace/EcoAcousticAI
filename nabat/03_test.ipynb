{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2861f6ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import glob\n",
    "import itertools\n",
    "import multiprocessing as mp\n",
    "import os.path\n",
    "import pickle\n",
    "import pprint\n",
    "import time\n",
    "from io import TextIOWrapper\n",
    "from multiprocessing import Manager, Pool, Process, Queue\n",
    "from os import path\n",
    "from zipfile import ZipFile\n",
    "\n",
    "import matplotlib.colors as colors\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from db import NABat_DB\n",
    "from PIL import Image\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "\n",
    "# Open the database connection.\n",
    "db = NABat_DB()\n",
    "\n",
    "# List the bat species we are considering in this model.\n",
    "species = db.query(\n",
    "    'select id, species_code, available from species where available = 1;')\n",
    "class_names = [c[1] for c in species]\n",
    "print(class_names)\n",
    "\n",
    "# Given a model and dataset, query our database to return pulse-level predictions.\n",
    "\n",
    "\n",
    "def get_predictions_pulse(model, draw):\n",
    "    pulses = db.fastQuery(\n",
    "        'select count(*) from file f join pulse p on p.file_id = f.id join species s on s.id = f.manual_id where draw = ? and available = 1 and f.grts_id != 0;', (draw,))\n",
    "    return db.fastQuery(\"\"\"\n",
    "    \n",
    "    with max_conf as (\n",
    "            select\n",
    "                p.pulse_id,\n",
    "                max(p.confidence) as confidence\n",
    "            from\n",
    "                prediction p\n",
    "            where \n",
    "                p.model_name = ?\n",
    "            group by\n",
    "                p.pulse_id\n",
    "        )\n",
    "        \n",
    "        select\n",
    "            f.id,\n",
    "            f.name,\n",
    "            s.species_code,\n",
    "            max_conf.confidence\n",
    "        from \n",
    "            max_conf\n",
    "        join\n",
    "            prediction p on p.pulse_id = max_conf.pulse_id and p.confidence = max_conf.confidence and p.model_name = ?\n",
    "        join \n",
    "            species s on s.id = p.species_id and available = 1\n",
    "        join \n",
    "            pulse on pulse.id = p.pulse_id\n",
    "        join\n",
    "            file f on f.id = pulse.file_id and f.grts_id != 0\n",
    "        join\n",
    "            species_grts sl on sl.grts_id = f.grts_id and sl.species_id = p.species_id\n",
    "        where \n",
    "            draw = ?\n",
    "    \"\"\", (model, model, draw)), pulses[0][0]\n",
    "\n",
    "\n",
    "# Given a model and dataset, query our database to return pulse-level predictions\n",
    "# aggregated up to the file level, using only pulse predictions with a confidence > min_confidence.\n",
    "def get_predictions_file(model, draw, min_confidence=0):\n",
    "    files = db.fastQuery(\n",
    "        'select count(*) from file f join species s on s.id = f.manual_id where draw = ? and available = 1 and f.grts_id != 0;', (draw,))\n",
    "\n",
    "    return db.fastQuery(\"\"\"\n",
    "    \n",
    "        with samples as (\n",
    "            select\n",
    "                file_id,\n",
    "                count(*) pc\n",
    "            from \n",
    "                pulse p\n",
    "            group by 1\n",
    "        ) \n",
    "        , conf as (\n",
    "            select\n",
    "                p.file_id,\n",
    "                pp.species_id,\n",
    "                count(p.id) num_samples,\n",
    "                sum(confidence)/(pc) sc\n",
    "            from\n",
    "                pulse p\n",
    "            join\n",
    "                prediction pp on pp.pulse_id = p.id and pp.model_name = ?\n",
    "            join \n",
    "                samples on samples.file_id = p.file_id\n",
    "            join \n",
    "                species s on s.id = pp.species_id and available = 1\n",
    "             where \n",
    "                pp.confidence > ?\n",
    "            group by\n",
    "                p.file_id, pp.species_id\n",
    "        )\n",
    "        \n",
    "        , xx as (\n",
    "        select\n",
    "            conf.file_id,\n",
    "            name,\n",
    "            s.species_code,\n",
    "            max(conf.sc) conf,\n",
    "            f.grts_id,\n",
    "            s.id as species_id\n",
    "        from \n",
    "            conf\n",
    "        join\n",
    "            file f on f.id = conf.file_id and f.grts_id != 0\n",
    "        join \n",
    "            species s on s.id = conf.species_id and available = 1\n",
    "        \n",
    "        where \n",
    "            draw = ?\n",
    "        group by \n",
    "            1,2 \n",
    "            \n",
    "        )\n",
    "        \n",
    "    select * from xx \n",
    "        join\n",
    "          species_grts sl on sl.grts_id = xx.grts_id and sl.species_id = xx.species_id\n",
    "    \"\"\", (model, min_confidence, draw)), files[0][0]\n",
    "\n",
    "\n",
    "# Plot a confusion matrix.\n",
    "def plot_confusion_matrix(cm, class_names, i, draw):\n",
    "    \"\"\"\n",
    "    Returns a matplotlib figure containing the plotted confusion matrix.\n",
    "\n",
    "    Args:\n",
    "       cm (array, shape = [n, n]): a confusion matrix of integer classes\n",
    "       class_names (array, shape = [n]): String names of the integer classes\n",
    "    \"\"\"\n",
    "    figure = plt.figure(figsize=(24, 24))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "    plt.title(\"Identification Rate: {} M-{}\".format(draw, i))\n",
    "    # plt.colorbar()\n",
    "    tick_marks = np.arange(len(class_names))\n",
    "    plt.xticks(tick_marks, class_names, rotation=45)\n",
    "    plt.yticks(tick_marks, class_names)\n",
    "\n",
    "    # Normalize the confusion matrix.\n",
    "    cm = np.around(cm.astype('float') / cm.sum(axis=1)\n",
    "                   [:, np.newaxis], decimals=3)\n",
    "\n",
    "    # Use white text if squares are dark; otherwise black.\n",
    "    threshold = cm.max() / 1.5\n",
    "\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        color = \"white\" if cm[i, j] > 0.70 else \"black\"\n",
    "        if cm[i, j] > 0.001:\n",
    "            plt.text(j, i, cm[i, j], horizontalalignment=\"center\", color=color)\n",
    "        else:\n",
    "            plt.text(j, i, '0', horizontalalignment=\"center\", color=color)\n",
    "\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.ioff()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ff73ec",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# Define the model, dataset, and confidence threshold.\n",
    "draw = 'test'\n",
    "model = '1'\n",
    "thresh = 0.00\n",
    "\n",
    "# Store aggregated output.\n",
    "test_labels = []\n",
    "test_predictions = []\n",
    "incorrect_confidences = []\n",
    "correct_confidences = []\n",
    "\n",
    "predictions, pulse_count = get_predictions_pulse(model, draw)\n",
    "\n",
    "rejected = pulse_count - len(predictions)\n",
    "\n",
    "# Loop through each pulse prediction.\n",
    "for p in predictions:\n",
    "    if p[-1] >= thresh:\n",
    "        test_labels.append(p[1].split('/')[-2])\n",
    "        test_predictions.append(p[2])\n",
    "\n",
    "        if test_labels[-1] == test_predictions[-1]:\n",
    "            correct_confidences.append(p[-1])\n",
    "        else:\n",
    "            incorrect_confidences.append(p[-1])\n",
    "    else:\n",
    "        rejected += 1\n",
    "\n",
    "print('Rejected {}/{} - {:0.2f}'.format(rejected,\n",
    "      pulse_count, 100 * (rejected/pulse_count)))\n",
    "\n",
    "\n",
    "# Print the chart.\n",
    "cr = classification_report(\n",
    "    test_labels, test_predictions,  labels=class_names, output_dict=False)\n",
    "print(cr, '\\n\\n')\n",
    "\n",
    "# Plot the matrix.\n",
    "cr = classification_report(\n",
    "    test_labels, test_predictions, labels=class_names, output_dict=True)\n",
    "cr_sorted_keys = sorted(class_names, key=lambda x: (\n",
    "    cr[x]['recall']), reverse=True)\n",
    "\n",
    "cm = metrics.confusion_matrix(\n",
    "    test_labels, test_predictions, labels=cr_sorted_keys, normalize='true')\n",
    "\n",
    "plot_confusion_matrix(cm, class_names=cr_sorted_keys,\n",
    "                      i=model, draw=draw.upper())\n",
    "\n",
    "\n",
    "# Plot the curve, based on confidence and split of correct versus wrong guesses.\n",
    "plt.close('all')\n",
    "figure = plt.figure(figsize=(20, 10))\n",
    "\n",
    "sum_wrong = []\n",
    "sum_correct = []\n",
    "x = 0\n",
    "moving_rate = 0.01\n",
    "while x < 1:\n",
    "    x += moving_rate\n",
    "    sum_wrong.append(len([v for v in incorrect_confidences if v < x]))\n",
    "    sum_correct.append(len([v for v in correct_confidences if v < x]))\n",
    "\n",
    "for i in range(len(sum_wrong)):\n",
    "    if sum_correct[i] > sum_wrong[i]:\n",
    "        break\n",
    "\n",
    "auc = i/100\n",
    "w1 = np.median(incorrect_confidences)\n",
    "c1 = np.median(correct_confidences)\n",
    "\n",
    "\n",
    "bins = np.linspace(0, 1, 40)\n",
    "\n",
    "plt.hist(correct_confidences, bins, alpha=0.5, label='Correct Prediction')\n",
    "plt.hist(incorrect_confidences, bins, alpha=0.5, label='Wrong Prediction')\n",
    "\n",
    "plt.vlines(w1, 0, 500, colors='orange', label='Median {:.3f}'.format(w1))\n",
    "plt.vlines(c1, 0, 500, colors='blue', label='Median {:.3f}'.format(c1))\n",
    "plt.vlines(auc, 0, 500, linestyles='dashed', colors='red',\n",
    "           label='Optimal AUC  {:.3f}'.format(auc))\n",
    "\n",
    "plt.ylabel('Count')\n",
    "plt.xlabel('Confidence Rate')\n",
    "plt.title('Validation Set Confidence Rate')\n",
    "\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b8cc70",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Define the model, dataset, and confidence threshold.\n",
    "draw = 'test'\n",
    "model = '1'\n",
    "thresh = 0.00\n",
    "\n",
    "# Store aggregated output.\n",
    "test_labels = []\n",
    "test_predictions = []\n",
    "incorrect_confidences = []\n",
    "correct_confidences = []\n",
    "\n",
    "predictions, file_count = get_predictions_file(model, draw, auc)\n",
    "rejected = file_count - len(predictions)\n",
    "\n",
    "# Loop through each pulse prediction.\n",
    "for p in predictions:\n",
    "    if p[3] >= thresh:\n",
    "        test_labels.append(p[1].split('/')[-2])\n",
    "        test_predictions.append(p[2])\n",
    "\n",
    "        if test_labels[-1] == test_predictions[-1]:\n",
    "            correct_confidences.append(p[3])\n",
    "        else:\n",
    "            incorrect_confidences.append(p[3])\n",
    "    else:\n",
    "        rejected += 1\n",
    "\n",
    "print('Rejected {}/{} - {:0.2f}'.format(rejected,\n",
    "      file_count, 100 * (rejected/file_count)))\n",
    "\n",
    "# Print the matrix.\n",
    "cr = classification_report(\n",
    "    test_labels, test_predictions,  labels=class_names, output_dict=False)\n",
    "print(cr, '\\n\\n')\n",
    "\n",
    "# Plot the matrix.\n",
    "cr = classification_report(\n",
    "    test_labels, test_predictions, labels=class_names, output_dict=True)\n",
    "\n",
    "cr_sorted_keys = sorted(class_names, key=lambda x: (\n",
    "    cr[x]['recall']), reverse=True)\n",
    "\n",
    "cm = metrics.confusion_matrix(\n",
    "    test_labels, test_predictions, labels=cr_sorted_keys, normalize='true')\n",
    "plot_confusion_matrix(cm, class_names=cr_sorted_keys,\n",
    "                      i=model, draw=draw.upper())\n",
    "\n",
    "\n",
    "# Plot the curve, based on confidence and split of correct versus wrong guesses.\n",
    "plt.close('all')\n",
    "figure = plt.figure(figsize=(20, 10))\n",
    "\n",
    "sum_wrong = []\n",
    "sum_correct = []\n",
    "x = 0\n",
    "moving_rate = 0.01\n",
    "while x < 1:\n",
    "    x += moving_rate\n",
    "    sum_wrong.append(len([v for v in incorrect_confidences if v < x]))\n",
    "    sum_correct.append(len([v for v in correct_confidences if v < x]))\n",
    "\n",
    "for i in range(len(sum_wrong)):\n",
    "    if sum_correct[i] > sum_wrong[i]:\n",
    "        break\n",
    "\n",
    "auc = i/100\n",
    "w1 = np.median(incorrect_confidences)\n",
    "c1 = np.median(correct_confidences)\n",
    "\n",
    "\n",
    "bins = np.linspace(0, 1, 40)\n",
    "\n",
    "plt.hist(correct_confidences, bins, alpha=0.5, label='Correct Prediction')\n",
    "plt.hist(incorrect_confidences, bins, alpha=0.5, label='Wrong Prediction')\n",
    "\n",
    "plt.vlines(w1, 0, 500, colors='orange', label='Median {:.3f}'.format(w1))\n",
    "plt.vlines(c1, 0, 500, colors='blue', label='Median {:.3f}'.format(c1))\n",
    "plt.vlines(auc, 0, 500, linestyles='dashed', colors='red',\n",
    "           label='Optimal AUC  {:.3f}'.format(auc))\n",
    "\n",
    "plt.ylabel('Count')\n",
    "plt.xlabel('Confidence Rate')\n",
    "plt.title('Validation Set Confidence Rate')\n",
    "\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
